{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7760c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import float32 as tffloat32\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow import newaxis, cast\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import VenusaurusTransformer\n",
    "import VenusaurusFileHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef52f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ae0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters for the training\n",
    "\n",
    "# To create profiles - TODO (actually add them to the function call)\n",
    "binWidth_l = 0.5    # Units cm - wire pitch?\n",
    "targetNBins_l = 50  # This equates to 25cm in length\n",
    "binWidth_t = 0.5    # Units cm - wire pitch?\n",
    "targetNBins_t = 20  # This equates to 10cm in length - moliere radius\n",
    "\n",
    "# To turn profiles into integer tokens - I think l and t should be the same?\n",
    "maxEnergyValue_l = 0.009\n",
    "nEnergyBins_l = 100\n",
    "energyBinWidth_l = float(maxEnergyValue_l) / float(nEnergyBins_l)\n",
    "\n",
    "maxEnergyValue_t = 0.22\n",
    "nEnergyBins_t = 500\n",
    "energyBinWidth_t = float(maxEnergyValue_t) / float(nEnergyBins_t)\n",
    "\n",
    "# Transformer parameters\n",
    "nVocab_l = nEnergyBins_l + 2 # Number of bins for energy depositions (transformer expects an integer tokens)\n",
    "nVocab_t = nEnergyBins_t + 2 # Number of bins for energy depositions (transformer expects an integer tokens)\n",
    "embedDim_l = 32              # Position embedding dimensions\n",
    "embedDim_t = 50              # Position embedding dimensions\n",
    "sequenceLength_l = targetNBins_l * int(2)\n",
    "sequenceLength_t = targetNBins_t\n",
    "\n",
    "nClasses = 5        # Number of types for classification\n",
    "\n",
    "nEpochs = 20         # Number of epochs to train for\n",
    "batchSize = 64     # Batch size\n",
    "learningRate = 1e-4 # Initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16167e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nutau/venusaurus_nutau_0.npz', '/Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nu/venusaurus_nu_1.npz', '/Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nu/venusaurus_nu_0.npz', '/Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nue/venusaurus_nue_0.npz', '/Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nue/venusaurus_nue_1.npz']\n",
      "Reading file:  /Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nutau/venusaurus_nutau_0.npz , This may take a while...\n",
      "Reading file:  /Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nu/venusaurus_nu_1.npz , This may take a while...\n",
      "Reading file:  /Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nu/venusaurus_nu_0.npz , This may take a while...\n",
      "Reading file:  /Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nue/venusaurus_nue_0.npz , This may take a while...\n",
      "Reading file:  /Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/nue/venusaurus_nue_1.npz , This may take a while...\n"
     ]
    }
   ],
   "source": [
    "# Here we'll get our information...\n",
    "\n",
    "# Profiles\n",
    "longProfiles_start_train = np.empty((0, targetNBins_l, 1))\n",
    "longProfiles_end_train = np.empty((0, targetNBins_l, 1))\n",
    "transProfiles_train = np.empty((0, targetNBins_t, 1))\n",
    "\n",
    "longProfiles_start_test = np.empty((0, targetNBins_l, 1))\n",
    "longProfiles_end_test = np.empty((0, targetNBins_l, 1))\n",
    "transProfiles_test = np.empty((0, targetNBins_t, 1))\n",
    "\n",
    "# Truth\n",
    "y_train = np.empty((0, nClasses))\n",
    "y_test = np.empty((0, nClasses))\n",
    "\n",
    "# Get training file(s)\n",
    "trainFileNames = glob.glob('/Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/*/*.npz')\n",
    "print(trainFileNames)\n",
    "\n",
    "for trainFileName in trainFileNames :\n",
    "    print('Reading file: ', str(trainFileName),', This may take a while...')\n",
    "    \n",
    "    data = np.load(trainFileName)\n",
    "\n",
    "    # Profiles\n",
    "    longProfiles_start_train =  np.concatenate((longProfiles_start_train, data['longProfiles_start_train']), axis=0)\n",
    "    longProfiles_end_train = np.concatenate((longProfiles_end_train, data['longProfiles_end_train']), axis=0)\n",
    "    transProfiles_train = np.concatenate((transProfiles_train, data['transProfiles_train']), axis=0)\n",
    "                           \n",
    "    longProfiles_start_test =  np.concatenate((longProfiles_start_test, data['longProfiles_start_test']), axis=0)\n",
    "    longProfiles_end_test = np.concatenate((longProfiles_end_test, data['longProfiles_end_test']), axis=0)\n",
    "    transProfiles_test = np.concatenate((transProfiles_test, data['transProfiles_test']), axis=0)\n",
    "\n",
    "    # Truth\n",
    "    y_train = np.concatenate((y_train, data['y_train']), axis=0)\n",
    "    y_test = np.concatenate((y_test, data['y_test']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f3d5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longProfiles_start_train:  (529214, 50, 1)\n",
      "longProfiles_end_train:  (529214, 50, 1)\n",
      "transProfiles_train:  (529214, 20, 1)\n",
      "longProfiles_start_test:  (58799, 50, 1)\n",
      "longProfiles_end_test:  (58799, 50, 1)\n",
      "transProfiles_test:  (58799, 20, 1)\n",
      "y_train:  (529214, 5)\n",
      "y_test (58799, 5)\n"
     ]
    }
   ],
   "source": [
    "# check everything went smoothly\n",
    "print('longProfiles_start_train: ', longProfiles_start_train.shape)\n",
    "print('longProfiles_end_train: ', longProfiles_end_train.shape)\n",
    "print('transProfiles_train: ', transProfiles_train.shape)\n",
    "\n",
    "print('longProfiles_start_test: ', longProfiles_start_test.shape)\n",
    "print('longProfiles_end_test: ', longProfiles_end_test.shape)\n",
    "print('transProfiles_test: ', transProfiles_test.shape)\n",
    "\n",
    "# Truth\n",
    "print('y_train: ', y_train.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bca3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a6720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af75d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To turn profiles into integer tokens\n",
    "\n",
    "zeroComparison = 0.00001 \n",
    "\n",
    "# longProfiles_start_train\n",
    "ls_train_mask_above = longProfiles_start_train > maxEnergyValue_l        # Mark those that surpase upper limit\n",
    "ls_train_mask_zero = longProfiles_start_train < zeroComparison           # Mark the padded or 'no value' tokens\n",
    "longProfiles_start_train = np.floor(longProfiles_start_train / energyBinWidth_l).astype('int64')\n",
    "longProfiles_start_train[ls_train_mask_above] = int(-1)                  # 1 typically marks OOB tokens\n",
    "longProfiles_start_train[ls_train_mask_zero] = int(-2)                   # 0 typically marks 'no value' tokens\n",
    "longProfiles_start_train = longProfiles_start_train + 2\n",
    "longProfiles_start_train[longProfiles_start_train > nVocab_l] = nVocab_l # Just to make sure (.f precision)\n",
    "\n",
    "# longProfiles_end_train\n",
    "le_train_mask_above = longProfiles_end_train > maxEnergyValue_l          # Mark those that surpase upper limit\n",
    "le_train_mask_zero = longProfiles_end_train < zeroComparison             # Mark the padded or 'no value' tokens\n",
    "longProfiles_end_train = np.floor(longProfiles_end_train / energyBinWidth_l).astype('int64')\n",
    "longProfiles_end_train[le_train_mask_above] = int(-1)                    # 1 typically marks OOB tokens\n",
    "longProfiles_end_train[le_train_mask_zero] = int(-2)                     # 0 typically marks 'no value' tokens\n",
    "longProfiles_end_train = longProfiles_end_train + 2\n",
    "longProfiles_end_train[longProfiles_end_train > nVocab_l] = nVocab_l     # Just to make sure (.f precision)\n",
    "\n",
    "# transProfiles_train\n",
    "t_train_mask_above = transProfiles_train > maxEnergyValue_t              # Mark those that surpase upper limit\n",
    "t_train_mask_zero = transProfiles_train < zeroComparison                 # Mark the padded or 'no value' tokens\n",
    "transProfiles_train = np.floor(transProfiles_train / energyBinWidth_t).astype('int64')\n",
    "transProfiles_train[t_train_mask_above] = int(-1)                        # 1 typically marks OOB tokens\n",
    "transProfiles_train[t_train_mask_zero] = int(-2)                         # 0 typically marks 'no value' tokens\n",
    "transProfiles_train = transProfiles_train + 2\n",
    "transProfiles_train[transProfiles_train > nVocab_t] = nVocab_t           # Just to make sure (.f precision)\n",
    "\n",
    "# longProfiles_start_test\n",
    "ls_test_mask_above = longProfiles_start_test > maxEnergyValue_l          # Mark those that surpase upper limit\n",
    "ls_test_mask_zero = longProfiles_start_test < zeroComparison             # Mark the padded or 'no value' tokens\n",
    "longProfiles_start_test = np.floor(longProfiles_start_test / energyBinWidth_l).astype('int64')\n",
    "longProfiles_start_test[ls_test_mask_above] = int(-1)                    # 1 typically marks OOB tokens\n",
    "longProfiles_start_test[ls_test_mask_zero] = int(-2)                     # 0 typically marks 'no value' tokens\n",
    "longProfiles_start_test = longProfiles_start_test + 2\n",
    "longProfiles_start_test[longProfiles_start_test > nVocab_l] = nVocab_l   # Just to make sure (.f precision)\n",
    "\n",
    "# longProfiles_end_test\n",
    "le_test_mask_above = longProfiles_end_test > maxEnergyValue_l            # Mark those that surpase upper limit\n",
    "le_test_mask_zero = longProfiles_end_test < zeroComparison               # Mark the padded or 'no value' tokens\n",
    "longProfiles_end_test = np.floor(longProfiles_end_test / energyBinWidth_l).astype('int64')\n",
    "longProfiles_end_test[le_test_mask_above] = int(-1)                      # 1 typically marks OOB tokens\n",
    "longProfiles_end_test[le_test_mask_zero] = int(-2)                       # 0 typically marks 'no value' tokens\n",
    "longProfiles_end_test = longProfiles_end_test + 2\n",
    "longProfiles_end_test[longProfiles_end_test > nVocab_l] = nVocab_l       # Just to make sure (.f precision)\n",
    "\n",
    "# transProfiles_test\n",
    "t_test_mask_above = transProfiles_test > maxEnergyValue_t             # Mark those that surpase upper limit\n",
    "t_test_mask_zero = transProfiles_test < zeroComparison                # Mark the padded or 'no value' tokens\n",
    "transProfiles_test = np.floor(transProfiles_test / energyBinWidth_t).astype('int64')\n",
    "transProfiles_test[t_test_mask_above] = int(-1)                       # 1 typically marks OOB tokens\n",
    "transProfiles_test[t_test_mask_zero] = int(-2)                        # 0 typically marks 'no value' tokens\n",
    "transProfiles_test = transProfiles_test + 2\n",
    "transProfiles_test[transProfiles_test > nVocab_l] = nVocab_l          # Just to make sure (.f precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "244ff6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine start and end profiles\n",
    "longProfiles_train = np.concatenate((longProfiles_start_train, longProfiles_end_train), axis=1)\n",
    "longProfiles_test = np.concatenate((longProfiles_start_test, longProfiles_end_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f30da84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longProfiles_train:  (529214, 100, 1)\n",
      "transProfiles_train:  (529214, 20, 1)\n",
      "longProfiles_test:  (58799, 100, 1)\n",
      "transProfiles_test:  (58799, 20, 1)\n",
      "y_train:  (529214, 5)\n",
      "y_test (58799, 5)\n"
     ]
    }
   ],
   "source": [
    "# Check everything went smoothly\n",
    "\n",
    "# Profiles\n",
    "print('longProfiles_train: ', longProfiles_train.shape)\n",
    "print('transProfiles_train: ', transProfiles_train.shape)\n",
    "\n",
    "print('longProfiles_test: ', longProfiles_test.shape)\n",
    "print('transProfiles_test: ', transProfiles_test.shape)\n",
    "\n",
    "# Truth\n",
    "print('y_train: ', y_train.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "289e5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape, because transformers are weird\n",
    "\n",
    "longProfiles_train = longProfiles_train.reshape(longProfiles_train.shape[0], sequenceLength_l)\n",
    "transProfiles_train = transProfiles_train.reshape(transProfiles_train.shape[0], sequenceLength_t)\n",
    "\n",
    "longProfiles_test = longProfiles_test.reshape(longProfiles_test.shape[0], sequenceLength_l)\n",
    "transProfiles_test = transProfiles_test.reshape(transProfiles_test.shape[0], sequenceLength_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46499f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " positional_embedding (Posi  (None, 100, 32)           3264      \n",
      " tionalEmbedding)                                                \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, 100, 32)           27424     \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 32)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30853 (120.52 KB)\n",
      "Trainable params: 30853 (120.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "venusaurusModel = VenusaurusTransformer.TransformerModel(sequenceLength_l, nVocab_l, nClasses, embedDim_l)\n",
    "venusaurusModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caddab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Define the optimiser and compile the model\n",
    "optimiser = Adam(learning_rate=learningRate)\n",
    "venusaurusModel.compile(loss='categorical_crossentropy', optimizer=optimiser, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc53364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: \n",
      "{0: 2.521015514809591, 1: 1.0, 2: 1.2476615943040625, 3: 2.7355371900826446, 4: 1.3775722543352602}\n"
     ]
    }
   ],
   "source": [
    "# Create class weights\n",
    "\n",
    "indexVector = np.argmax(y_test, axis=1)\n",
    "\n",
    "# muons = 0, protons = 1, pions = 2, electrons = 3, photons = 4, other = 5\n",
    "\n",
    "nMuons = np.count_nonzero(indexVector == 0)    \n",
    "nProtons = np.count_nonzero(indexVector == 1)  \n",
    "nPions = np.count_nonzero(indexVector == 2)  \n",
    "nElectrons = np.count_nonzero(indexVector == 3)  \n",
    "nPhotons = np.count_nonzero(indexVector == 4)  \n",
    "\n",
    "# Normalise to largest\n",
    "maxParticle = max(nMuons, nProtons, nPions, nElectrons, nPhotons)\n",
    "\n",
    "classWeights = {0: maxParticle/nMuons, 1: maxParticle/nProtons, 2: maxParticle/nPions, 3: maxParticle/nElectrons, 4: maxParticle/nPhotons}\n",
    "\n",
    "#classWeights = {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}\n",
    "\n",
    "\n",
    "print('Class Weights: ')\n",
    "print(classWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3022e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8269/8269 [==============================] - ETA: 0s - loss: 1.7465 - accuracy: 0.5119\n",
      "Epoch 1: val_accuracy improved from -inf to 0.65306, saving model to /Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/test\n",
      "INFO:tensorflow:Assets written to: /Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8269/8269 [==============================] - 426s 51ms/step - loss: 1.7465 - accuracy: 0.5119 - val_loss: 0.8381 - val_accuracy: 0.6531 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "2845/8269 [=========>....................] - ETA: 4:35 - loss: 1.2834 - accuracy: 0.6483"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "checkpoint = ModelCheckpoint('/Users/isobel/Desktop/DUNE/Ivysaurus/Venusaurus/files/test', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Reduce the learning rate by a factor of ten when required\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-6, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = venusaurusModel.fit(longProfiles_train, y_train, \n",
    "    batch_size = batchSize, validation_data=(longProfiles_test, y_test), \n",
    "    shuffle=True, epochs=nEpochs, class_weight=classWeights, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the network to predict the category of the test sample\n",
    "y_pred = venusaurusModel.predict(longProfiles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cdd4f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "trueSums = np.sum(conf_matrix, axis=1)\n",
    "predSums = np.sum(conf_matrix, axis=0)\n",
    "\n",
    "trueNormalised = np.zeros(shape=(nClasses, nClasses))\n",
    "predNormalised = np.zeros(shape=(nClasses, nClasses))\n",
    "\n",
    "for trueIndex in range(nClasses) : \n",
    "    for predIndex in range(nClasses) :\n",
    "        nEntries = conf_matrix[trueIndex][predIndex]\n",
    "        trueNormalised[trueIndex][predIndex] = float(nEntries) / float(trueSums[trueIndex])\n",
    "        predNormalised[trueIndex][predIndex] = nEntries / predSums[predIndex]\n",
    "\n",
    "\n",
    "displayTrueNorm = ConfusionMatrixDisplay(confusion_matrix=trueNormalised, display_labels=[\"Muon\", \"Proton\", \"Pion\", \"Electron\", \"Photon\"])\n",
    "displayTrueNorm.plot()\n",
    "\n",
    "displayPredNorm = ConfusionMatrixDisplay(confusion_matrix=predNormalised, display_labels=[\"Muon\", \"Proton\", \"Pion\", \"Electron\", \"Photon\"])\n",
    "displayPredNorm.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f6783",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "bkgRej = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(nClasses):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    bkgRej[i] = 1 - fpr[i]\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot of a ROC curve for a specific class\n",
    "for i in range(nClasses):\n",
    "    plt.figure()\n",
    "    plt.plot(tpr[i], bkgRej[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xticks(np.arange(0, 1, 0.1))\n",
    "    plt.yticks(np.arange(0, 1, 0.1))\n",
    "    plt.xlabel('Efficiency')\n",
    "    plt.ylabel('BG Rejection')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_colors = ('red', 'green', 'blue')\n",
    "hist_titles = ('CNN Muon Score', 'CCN Pion Score', 'CNN Proton Score')\n",
    "\n",
    "for i in range(nClasses) :\n",
    "    \n",
    "    if (i > 2) :\n",
    "        continue\n",
    "    \n",
    "    for j in range(nClasses) :\n",
    "    \n",
    "        if (j > 2) :\n",
    "            continue\n",
    "    \n",
    "        nTrueParticles = y_pred[y_test[:,j] == 1].shape[0]\n",
    "        weights = np.full(nTrueParticles, 1.0/nTrueParticles)\n",
    "        plt.hist(y_pred[y_test[:,j] == 1][:,i], bins=40, weights=weights, color=particle_colors[j], histtype='step')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.xlabel(hist_titles[i])\n",
    "        plt.ylabel('Proportion of Tracks')\n",
    "        plt.legend(['Muon', 'Pion', 'Proton'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2c840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef4d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d059d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba88f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bd931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work out normalisation...\n",
    "\n",
    "\n",
    "\n",
    "longProfiles_start_train = longProfiles_start_train.reshape(-1)\n",
    "longProfiles_end_train = longProfiles_end_train.reshape(-1)\n",
    "transProfiles_train = transProfiles_train.reshape(-1)\n",
    "\n",
    "print(longProfiles_start_train.shape)\n",
    "\n",
    "longProfiles_start_train = longProfiles_start_train[longProfiles_start_train > 0.000001]\n",
    "longProfiles_end_train = longProfiles_end_train[longProfiles_end_train > 0.000001]\n",
    "transProfiles_train = transProfiles_train[transProfiles_train > 0.000001]\n",
    "\n",
    "long = np.concatenate((longProfiles_start_train, longProfiles_end_train))\n",
    "\n",
    "print('longProfiles_start_train: ', np.percentile(longProfiles_start_train, 95.0))\n",
    "print('longProfiles_end_train: ', np.percentile(longProfiles_end_train, 95.0))\n",
    "print('long: ', np.percentile(long, 95))\n",
    "print('transProfiles_train: ', np.percentile(transProfiles_train, 95.0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22801d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
